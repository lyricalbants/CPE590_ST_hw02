{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emmanuel Ezeobidi\n",
    "#### eie0002@uah.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given {(1,2),(2,1),(4,5),(5,4)} with initial centroids at (1,1) and (5,5), to determine the points and number of clusters:\n",
    "\\begin{align*}\n",
    "    \\sqrt{(x_i-x_{c1})^2+(y_i-y_{c1})^2} \\lt \\sqrt{(x_i-x_{c2})^2+(y_i-y_{c2})^2}\n",
    "\\end{align*}\n",
    "\n",
    "From inspection, points (1,2) and (2,1) belong to cluster 1 and (4,5) and (5,4) belong to cluster 2. Recomputing centroids:\n",
    "\\begin{align*}\n",
    "    C_1 = \\biggl(\\frac{1+2}{2}, \\frac{2+1}{2}\\biggr) = (1.5, 1.5) \\\\\n",
    "    C_2 = \\biggl(\\frac{4+5}{2}, \\frac{5+4}{2}\\biggr) = (4.5, 4.5)\n",
    "\\end{align*}\n",
    "New centroids are (1.5, 1.5) and (4.5, 4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As $\\varepsilon$ increases, the number of clusters usually decreases. This is because $\\varepsilon$ defines the radius of the neighborhood around a point. When $\\varepsilon$ is small, only very close points are considered neighbors leading to many small clusters and more noise. When $\\varepsilon$ is large, distant points are considered neighbors and hence separate clusters merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We could use clustering techniques to detect anomalies in various ways:\n",
    "    * After clustering, calculate the distance of each point to its cluster center. Points far away from any cluster center are considered anomalies.\n",
    "    * In DBSCAN case, points in low-density regions are flagged as noise or outliers.\n",
    "    * Depending on how many points you define your cluster size as,  very small clusters with points smaller than the threshold could be treated as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Given\n",
    "* Input: $x_t = [0.5]$\n",
    "* Previous hidden state: $h_{t-1}=[0.3]$\n",
    "* Weights: $W_{xh} = [0.8]$ and $W_{hh} = [0.2]$\n",
    "* Bias: $b_h = [0.1]$\n",
    "Activation: $tanh$\n",
    "\n",
    "The RNN hidden state update formula:\n",
    "\\begin{align*}\n",
    "    h_t = tanh(W_{sh}x_t + W_{hh}h_{t-1} + b_h)\n",
    "\\end{align*}\n",
    "Substituing the given values:\n",
    "\\begin{align*}\n",
    "    h_t &= tanh((0.8\\times0.5)+(0.2\\times0.3)+0.1) \\\\\n",
    "    &= tanh(0.56) \\approx \\boxed{0.508}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Given: \\\n",
    "\\\n",
    "$W_h = \n",
    "\\begin{bmatrix}\n",
    "    0.5 & -0.6\\\\\n",
    "    0.8 & -0.1\\\\\n",
    "\\end{bmatrix}$, \\\n",
    "\\\n",
    "$W_x = \n",
    "\\begin{bmatrix}\n",
    "    -0.2 \\\\\n",
    "    0.4 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$b_h = \n",
    "\\begin{bmatrix}\n",
    "    -0.1 \\\\\n",
    "    0.2 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$W_y = \n",
    "\\begin{bmatrix}\n",
    "    0.3 & -0.7 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$b_y = \n",
    "\\begin{bmatrix}\n",
    "    0.1 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$h_{t-1} = \n",
    "\\begin{bmatrix}\n",
    "    0.7 \\\\\n",
    "    -0.8 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$x_t = \n",
    "\\begin{bmatrix}\n",
    "    0.5 \\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "Formulas:\n",
    "\\begin{align*}\n",
    "    h_t = \\sigma (W_hh_{t-1}+W_xx_t+b_h) \\\\\n",
    "    y_t = W_yh_t + b_y \\\\\n",
    "    where \\: \\sigma \\: is \\: the \\: sigmoid \\: function: \\\\\n",
    "    \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{align*}\n",
    "\n",
    "We first compute\n",
    "$$W_hh_{t-1} = \\begin{bmatrix}\n",
    "                    0.5\\times 0.7 + (-0.6) \\times (-0.8) \\\\\n",
    "                    0.8\\times 0.7 + (-0.1) \\times (-0.8)\n",
    "                \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "                    0.83 \\\\\n",
    "                    0.64\n",
    "                \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$W_xx_t = \\begin{bmatrix}\n",
    "                    -0.2\\times 0.5 \\\\\n",
    "                    0.4 \\times 0.5\n",
    "                \\end{bmatrix}\n",
    "            = \\begin{bmatrix}\n",
    "                    -0.1 \\\\\n",
    "                    0.2\n",
    "                \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "W_hh_{t-1} + W_xx_t + b_h \\\\\n",
    "\n",
    "            = \\begin{bmatrix}\n",
    "                    0.83 \\\\\n",
    "                    0.64\n",
    "                    \\end{bmatrix}\n",
    "            +\n",
    "               \\begin{bmatrix}\n",
    "                    -0.1 \\\\\n",
    "                    0.2\n",
    "                \\end{bmatrix}\n",
    "            +\n",
    "                 \\begin{bmatrix}\n",
    "                    -0.1 \\\\\n",
    "                    0.2\n",
    "                \\end{bmatrix}\n",
    "            =\n",
    "                \\begin{bmatrix}\n",
    "                    0.83 - 0.1 - 0.1 \\\\\n",
    "                    0.64 + 0.2 + 0.2\n",
    "                \\end{bmatrix}\n",
    "\n",
    "            =   \\begin{bmatrix}\n",
    "                    0.63 \\\\\n",
    "                    1.04\n",
    "                \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = \\sigma \\biggl( \\begin{bmatrix}\n",
    "                    0.63 \\\\\n",
    "                    1.04\n",
    "                \\end{bmatrix}\\biggr) \n",
    "            =   \\begin{bmatrix}\n",
    "                    \\sigma(0.63) \\\\\n",
    "                    \\sigma(1.04) \n",
    "                \\end{bmatrix} \n",
    "            = \\boxed{\\begin{bmatrix}\n",
    "                    0.652 \\\\\n",
    "                    0.739\n",
    "                \\end{bmatrix}}\n",
    "$$ \n",
    "And $y_t$ is\n",
    "$$\n",
    "y_t = W_yh_t + b_y \\\\\n",
    "    = 0.3 \\times 0.652 + (-0.7) \\times 0.739 + 0.1 \\\\\n",
    "    = 0.1956 - 0.5137 + 0.1 \\\\\n",
    "\\boxed{y_t = - 0.2217}\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Given: \\\n",
    "\\\n",
    "$Q = \n",
    "\\begin{bmatrix}\n",
    "    1 & 2\\\\\n",
    "    3 & 4\\\\\n",
    "\\end{bmatrix}$, \\\n",
    "\\\n",
    "$K = \n",
    "\\begin{bmatrix}\n",
    "    2 & 1\\\\\n",
    "    4 & 3\\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "$V = \n",
    "\\begin{bmatrix}\n",
    "    0.5 & 1.0\\\\\n",
    "    1.5 & 2.0\\\\\n",
    "\\end{bmatrix}$ \\\n",
    "\\\n",
    "Formulas:\n",
    "\\begin{align*}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V \\\\\n",
    "    d_k = 2\n",
    "\\end{align*}\n",
    "\n",
    "First, \n",
    "$$\n",
    "K^\\top =\n",
    "\\begin{bmatrix}\n",
    "    2 & 4\\\\\n",
    "    1 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "QK^\\top =\n",
    "\\begin{bmatrix}\n",
    "    1\\times2 + 2\\times1 & 1\\times4 + 2\\times3\\\\\n",
    "    3\\times2 + 4\\times1 & 3\\times4 + 4\\times3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    4 & 10\\\\\n",
    "    10 & 24\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Scaling by $\\sqrt{d_k} = \\sqrt{2} \\approx 1.414$:\n",
    "\n",
    "$$\n",
    "\\frac{QK^\\top}{\\sqrt{2}} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{4}{1.414} & \\frac{10}{1.414} \\\\\n",
    "    \\frac{10}{1.414} & \\frac{24}{1.414}\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "    2.828 & 7.07 \\\\\n",
    "    7.07 & 16.97\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply softmax **row-wise**:\n",
    "\n",
    "First row:\n",
    "\\begin{align*}\n",
    "    \\text{softmax}(2.828, 7.07) &= \\left( \\frac{e^{2.828}}{e^{2.828} + e^{7.07}}, \\frac{e^{7.07}}{e^{2.828} + e^{7.07}} \\right) \\\\\n",
    "    &\\approx (0.014, 0.986)\n",
    "\\end{align*}\n",
    "\n",
    "Second row:\n",
    "\\begin{align*}\n",
    "    \\text{softmax}(7.07, 16.97) &\\approx (0, 1)\n",
    "\\end{align*}\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{2}} \\right) \\approx\n",
    "\\begin{bmatrix}\n",
    "    0.014 & 0.986 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \n",
    "\\begin{bmatrix}\n",
    "    0.014 & 0.986 \\\\\n",
    "    0 & 1\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "    0.5 & 1.0 \\\\\n",
    "    1.5 & 2.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First row:\n",
    "\\begin{align*}\n",
    "    (0.014\\times0.5) + (0.986\\times1.5) &= 1.486 \\\\\n",
    "    (0.014\\times1.0) + (0.986\\times2.0) &= 1.986\n",
    "\\end{align*}\n",
    "\n",
    "Second row:\n",
    "\\begin{align*}\n",
    "    (0\\times0.5) + (1\\times1.5) &= 1.5 \\\\\n",
    "    (0\\times1.0) + (1\\times2.0) &= 2.0\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the final output is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{bmatrix}\n",
    "    1.486 & 1.986\\\\\n",
    "    1.5 & 2.0\n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7.1 Steady-state distributions:\n",
      "  Action a: [0.42857143 0.44155844 0.12987013]\n",
      "  Action c: [0.35151515 0.38787879 0.26060606]\n",
      "\n",
      "Q7.2 Simulated steady-state distributions:\n",
      "  Action a: [0.43295 0.43825 0.1288 ]\n",
      "  Action c: [0.35084 0.38721 0.26195]\n",
      "Policy Iteration optimal policy: ['c', 'a', 'a']\n",
      "Value Iteration optimal policy:  ['c', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transition matrices for actions a (aggressive) and c (cautious)\n",
    "P_a = np.array([\n",
    "    [0.6, 0.2, 0.2],  # from s0\n",
    "    [0.3, 0.6, 0.1],  # from s1\n",
    "    [0.3, 0.7, 0.0]   # from s2\n",
    "])\n",
    "\n",
    "P_c = np.array([\n",
    "    [0.3, 0.4, 0.3],  # from s0\n",
    "    [0.5, 0.1, 0.4],  # from s1\n",
    "    [0.2, 0.8, 0.0]   # from s2\n",
    "])\n",
    "\n",
    "def steady_state(P):\n",
    "    \"\"\"\n",
    "    Compute the steady-state distribution π satisfying π = π P\n",
    "    by finding the left eigenvector of P with eigenvalue 1.\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eig(P.T)\n",
    "    idx = np.argmin(np.abs(eigvals - 1))\n",
    "    vec = np.real(eigvecs[:, idx])\n",
    "    return vec / vec.sum()\n",
    "\n",
    "def simulate_mdp(P, start_state=0, n_steps=100_000):\n",
    "    \"\"\"\n",
    "    Simulate the Markov chain for n_steps and estimate the\n",
    "    steady-state distribution empirically.\n",
    "    \"\"\"\n",
    "    counts = np.zeros(P.shape[0])\n",
    "    state = start_state\n",
    "    for _ in range(n_steps):\n",
    "        counts[state] += 1\n",
    "        state = np.random.choice(len(P), p=P[state])\n",
    "    return counts / n_steps\n",
    "\n",
    "\n",
    "# Q7.1: Analytical steady-state\n",
    "pi_a = steady_state(P_a)\n",
    "pi_c = steady_state(P_c)\n",
    "print(\"Q7.1 Steady-state distributions:\")\n",
    "print(\"  Action a:\", pi_a)\n",
    "print(\"  Action c:\", pi_c)\n",
    "\n",
    "# Q7.2: Empirical simulation\n",
    "sim_pi_a = simulate_mdp(P_a)\n",
    "sim_pi_c = simulate_mdp(P_c)\n",
    "print(\"\\nQ7.2 Simulated steady-state distributions:\")\n",
    "print(\"  Action a:\", sim_pi_a)\n",
    "print(\"  Action c:\", sim_pi_c)\n",
    "\n",
    "# 7.3: Optimal policy\n",
    "import numpy as np\n",
    "\n",
    "# Transition matrices for actions a (aggressive) and c (cautious)\n",
    "P = {\n",
    "    'a': np.array([\n",
    "        [0.6, 0.2, 0.2],  # from s0\n",
    "        [0.3, 0.6, 0.1],  # from s1\n",
    "        [0.3, 0.7, 0.0],  # from s2\n",
    "    ]),\n",
    "    'c': np.array([\n",
    "        [0.3, 0.4, 0.3],  # from s0\n",
    "        [0.5, 0.1, 0.4],  # from s1\n",
    "        [0.2, 0.8, 0.0],  # from s2\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Reward matrices R[action][state, next_state]\n",
    "\n",
    "R = {\n",
    "    'a': np.array([\n",
    "        [0, 0, 3],  # rewards for transitions from s0 under 'a'\n",
    "        [2, 2, 1],  # from s1 under 'a'\n",
    "        [ 2, 1, 0],  # from s2 under 'a'\n",
    "    ]),\n",
    "    'c': np.array([\n",
    "        [1, 1, -1],  # from s0 under 'c'\n",
    "        [ 0, 0, -1],  # from s1 under 'c'\n",
    "        [ 0, 1, 0],  # from s2 under 'c'\n",
    "    ])\n",
    "}\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "def policy_evaluation(policy, P, R, gamma, theta=1e-6):\n",
    "    n_states = P['a'].shape[0]\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            a = policy[s]\n",
    "            v = V[s]\n",
    "            V[s] = sum(P[a][s, s_next] * (R[a][s, s_next] + gamma * V[s_next])\n",
    "                       for s_next in range(n_states))\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_iteration(P, R, gamma=0.9):\n",
    "    n_states = P['a'].shape[0]\n",
    "    policy = ['a'] * n_states  # initialize to action 'a'\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, P, R, gamma)\n",
    "        stable = True\n",
    "        for s in range(n_states):\n",
    "            # find best action in state s\n",
    "            action_values = {}\n",
    "            for a in P:\n",
    "                action_values[a] = sum(P[a][s, s_next] * (R[a][s, s_next] + gamma * V[s_next])\n",
    "                                       for s_next in range(n_states))\n",
    "            best_a = max(action_values, key=action_values.get)\n",
    "            if best_a != policy[s]:\n",
    "                stable = False\n",
    "                policy[s] = best_a\n",
    "        if stable:\n",
    "            return policy, V\n",
    "\n",
    "def value_iteration(P, R, gamma=0.9, theta=1e-6):\n",
    "    n_states = P['a'].shape[0]\n",
    "    V = np.zeros(n_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            V[s] = max(\n",
    "                sum(P[a][s, s_next] * (R[a][s, s_next] + gamma * V[s_next])\n",
    "                    for s_next in range(n_states))\n",
    "                for a in P\n",
    "            )\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # derive policy\n",
    "    policy = []\n",
    "    for s in range(n_states):\n",
    "        action_values = {a: sum(P[a][s, s_next] * (R[a][s, s_next] + gamma * V[s_next])\n",
    "                                for s_next in range(n_states))\n",
    "                         for a in P}\n",
    "        policy.append(max(action_values, key=action_values.get))\n",
    "    return policy, V\n",
    "\n",
    "pi_policy, V_pi = policy_iteration(P, R, gamma)\n",
    "pi_value,  V_vi = value_iteration(P, R, gamma)\n",
    "\n",
    "print(\"Policy Iteration optimal policy:\", pi_policy)\n",
    "print(\"Value Iteration optimal policy: \", pi_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Metric       Score\n",
      "0      Adjusted Rand Index    0.277338\n",
      "1                V-measure    0.212245\n",
      "2         Silhouette Score    0.468925\n",
      "3  Calinski-Harabasz Index  664.078751\n"
     ]
    }
   ],
   "source": [
    "# 8 K_means\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Q8: Generate half-moon dataset\n",
    "X, y_true = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Compute clustering quality metrics\n",
    "ari = adjusted_rand_score(y_true, y_pred)\n",
    "vmeas = v_measure_score(y_true, y_pred)\n",
    "sil = silhouette_score(X, y_pred)\n",
    "chi = calinski_harabasz_score(X, y_pred)\n",
    "\n",
    "# Prepare results\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Adjusted Rand Index\",\n",
    "        \"V-measure\",\n",
    "        \"Silhouette Score\",\n",
    "        \"Calinski-Harabasz Index\"\n",
    "    ],\n",
    "    \"Score\": [ari, vmeas, sil, chi]\n",
    "})\n",
    "\n",
    "print(df_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
